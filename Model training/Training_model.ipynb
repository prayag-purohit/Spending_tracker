{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines a comprehensive approach to training a financial transaction categorization model using a pre-trained transformer model (distilbert-base-uncased). By leveraging **local processing** and privacy-focused techniques, the sensitive nature of bank transaction data is securely maintained. This method addresses class imbalance through oversampling and utilizes robust evaluation metrics to monitor performance.\n",
    "\n",
    "**Highlights**\n",
    "\n",
    "*Sensitive Data Handling*: Utilizes local processing with Hugging Face libraries to ensure data privacy.\n",
    "\n",
    "*Pre-trained Model*: Uses distilbert-base-uncased, benefiting from transfer learning for better performance on limited data.\n",
    "\n",
    "*Class Imbalance Handling*: Employs oversampling techniques to balance the dataset effectively.\n",
    "\n",
    "*Error Tracking*: Logs model errors to monitor and enhance accuracy over time.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "*Small Dataset Size*: The initial dataset contains only 300 rows, which may limit the modelâ€™s generalizability. This is because of the sensitive nature of bank transaction data. Finding good-quality open-source data is tough. I tried data augmentation techniques but conventional augmentation techqniques do not work because of the uniqueness of the bank data. \n",
    "\n",
    "*Overfitting Risk*: With a small dataset and a powerful model, there is a high risk of overfitting. Implementing more regularization techniques and careful monitoring of training and validation loss will help mitigate this issue.\n",
    "\n",
    "I can overcome these limitations by periodically retraining the model with new data, you can ensure that the model stays up-to-date with any new patterns or trends in your transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Oversampling\n",
    "the section focuses on data preparation and handling class imbalance. It involves loading the dataset, extracting unique values, defining a sampling strategy, and performing oversampling to ensure balanced categories for model training.\n",
    "\n",
    "Dataset is a very small dataset with only 300 trainable rows. The data is highly sensitive, so it is very hard to find an open-source large dataset. \n",
    "\n",
    "The sample model sequence would look like this:\n",
    "{Transaction note + : + $ Amount}\n",
    "\n",
    "For example: \n",
    "POS Purchase FPOS Beer Store C45051: $13.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data analytics projects\\Financial Tracker\\FinTrackerenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined</th>\n",
       "      <th>big_amount</th>\n",
       "      <th>category</th>\n",
       "      <th>category_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos purchase opos presto autl toron: $-20.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Public transport</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos purchase gpos hong's esso north: $-11.72</td>\n",
       "      <td>False</td>\n",
       "      <td>Ciggerates</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miscellaneous payment transferwise canada_____...</td>\n",
       "      <td>True</td>\n",
       "      <td>Income</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos purchase gpos freshchoice ind etobi: $-23.21</td>\n",
       "      <td>False</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matchbox cannabis - ste north york on: $-34.21</td>\n",
       "      <td>False</td>\n",
       "      <td>Weed</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            combined  big_amount  \\\n",
       "0        pos purchase opos presto autl toron: $-20.0       False   \n",
       "1       pos purchase gpos hong's esso north: $-11.72       False   \n",
       "2  miscellaneous payment transferwise canada_____...        True   \n",
       "3   pos purchase gpos freshchoice ind etobi: $-23.21       False   \n",
       "4     matchbox cannabis - ste north york on: $-34.21       False   \n",
       "\n",
       "           category  category_encoded  \n",
       "0  Public transport                11  \n",
       "1        Ciggerates                 4  \n",
       "2            Income                 8  \n",
       "3         Groceries                 7  \n",
       "4              Weed                18  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries and load the dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "p_df = pd.read_csv(\"transaction_data_feature_engineered (python).csv\")\n",
    "p_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts unique values from the DataFrame columns `category` and `category_encoded` to create a mapping dictionary (`classification_labels`). It also calculates the frequency of each category and stores it in `category_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{13: 46, 4: 41, 11: 32, 9: 29, 0: 20, 16: 20, 15: 17, 8: 16, 14: 16, 7: 13, 2: 13, 1: 12, 18: 11, 17: 6, 12: 6, 10: 6, 3: 5, 6: 4, 19: 2, 5: 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Public transport': 11,\n",
       " 'Ciggerates': 4,\n",
       " 'Income': 8,\n",
       " 'Groceries': 7,\n",
       " 'Weed': 18,\n",
       " 'Restaurant, fast-food': 13,\n",
       " 'Child Support': 3,\n",
       " 'Interact': 9,\n",
       " 'Charges, Fees': 2,\n",
       " 'Unknown': 17,\n",
       " 'Alcohol': 0,\n",
       " 'Shopping': 14,\n",
       " 'Wellness, beauty': 19,\n",
       " 'Bill payments, Subscriptions': 1,\n",
       " 'Rent': 12,\n",
       " 'TRANSFER': 15,\n",
       " 'Education, development': 6,\n",
       " 'Phone, cell phone': 10,\n",
       " 'Drug-store, chemist': 5,\n",
       " 'Taxi': 16}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract unique values for category and category_encoded\n",
    "unique_values = p_df[['category', 'category_encoded']]\n",
    "classification_labels = unique_values.set_index('category').to_dict()['category_encoded']\n",
    "\n",
    "# Count the number of occurrences for each category\n",
    "category_count_dict = p_df['category_encoded'].value_counts().to_dict()\n",
    "print(category_count_dict)\n",
    "classification_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique classification labels and load the access token\n",
    "len_unique_labels = len(classification_labels)\n",
    "\n",
    "print(len_unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling\n",
    "Oversampling the dataset to address class imbalances within the dataset. A simple strategy was employed after doing some EDA in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{13: 46, 4: 41, 11: 32, 9: 29, 0: 20, 16: 20, 15: 17, 8: 16, 14: 16, 7: 20, 2: 20, 1: 18, 18: 16, 17: 9, 12: 9, 10: 9, 3: 8, 6: 10, 19: 5, 5: 5}\n"
     ]
    }
   ],
   "source": [
    "# Define sampling strategy for oversampling\n",
    "\n",
    "import math\n",
    "sampling_strategy = {}\n",
    "for key, value in category_count_dict.items():\n",
    "    if value < 5:\n",
    "        sampling_strategy[key] = round(value * 2.5)\n",
    "    elif 5 <= value <= 14:\n",
    "        sampling_strategy[key] = round(value * 1.5)\n",
    "    else:\n",
    "        sampling_strategy[key] = round(value)\n",
    "\n",
    "print(sampling_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform oversampling to handle class imbalance\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "X = p_df['combined']\n",
    "X_reshaped = np.array(X).reshape(-1, 1)\n",
    "y = p_df['category_encoded']\n",
    "y_reshaped = np.array(y).reshape(-1, 1)\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy)\n",
    "X_resampled, y_resampled = ros.fit_resample(X=X_reshaped, y=y_reshaped)\n",
    "from collections import Counter\n",
    "resampled_counts = dict((Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key        Dict 1     Dict 2     Dict 3\n",
      "0          20         20         20\n",
      "1          18         18         12\n",
      "2          20         20         13\n",
      "3          8          8          5\n",
      "4          41         41         41\n",
      "5          5          5          2\n",
      "6          10         10         4\n",
      "7          20         20         13\n",
      "8          16         16         16\n",
      "9          29         29         29\n",
      "10         9          9          6\n",
      "11         32         32         32\n",
      "12         9          9          6\n",
      "13         46         46         46\n",
      "14         16         16         16\n",
      "15         17         17         17\n",
      "16         20         20         20\n",
      "17         9          9          6\n",
      "18         16         16         11\n",
      "19         5          5          2\n",
      "\n",
      "The dictionaries contain different elements.\n"
     ]
    }
   ],
   "source": [
    "# Defines a function to compare n number of dictionaries. \n",
    "# The function later is used to assess whether the class imbalance was addressed as per the sampling strategy\n",
    "def compare_dictionaries(*dictionaries):\n",
    "    \"\"\"\n",
    "    Compares multiple dictionaries and prints a side-by-side comparison table.\n",
    "\n",
    "    Args:\n",
    "        *dictionaries: A variable number of dictionaries to compare.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get all keys from all dictionaries\n",
    "    all_keys = set.union(*[set(d.keys()) for d in dictionaries])\n",
    "\n",
    "    # Sort the keys\n",
    "    sorted_keys = sorted(all_keys)\n",
    "\n",
    "    # Print the table header\n",
    "    print(\"{:<10} {:<10} {:<10} {}\".format(\"Key\", *[f\"Dict {i+1}\" for i in range(len(dictionaries))]))\n",
    "\n",
    "    # Print each key-value pair for all dictionaries\n",
    "    for key in sorted_keys:\n",
    "        values = [d.get(key, None) for d in dictionaries]\n",
    "        print(\"{:<10} {:<10} {:<10} {}\".format(key, *values))\n",
    "\n",
    "    # Check for same elements (order doesn't matter)\n",
    "    if all(sorted(d.items()) == sorted(dictionaries[0].items()) for d in dictionaries[1:]):\n",
    "        print(\"\\nThe dictionaries contain the same elements (order may differ).\")\n",
    "    else:\n",
    "        print(\"\\nThe dictionaries contain different elements.\")\n",
    "\n",
    "compare_dictionaries(resampled_counts, sampling_strategy, category_count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation and Training\n",
    "\n",
    "the section focuses on model preparation and training. It involves loading the pre-trained model and tokenizer, splitting and tokenizing the data, defining a custom Dataset class, freezing certain model layers, and setting up evaluation metrics.\n",
    "\n",
    "The model of choice is distilbert base-uncased. The model was selected because it is a relatively small model compared to other open source model, with only 260 million parameters. The small model size can be leveraged for smaller hardware. Furthermore, since it was trained on open-source wikipedia data, it could help in identifying brand name tokens. For example, A token with *Beer Store* would be closer to *Alocohol* in the mappings of the model. \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv() # Load environment variables\n",
    "access_token = os.getenv(\"HUGGINGFACETOKEN\")\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=20)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", token=access_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resampled data is split into training and testing sets with 10% of the data reserved for testing. The text data is then tokenized using the pre-trained tokenizer with padding and truncation to a maximum length. Test size was kept smaller because of the small size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 29215, 5092, 1013, 10546, 1001, 6021, 2575, 2475, 3802, 16429, 11261, 3489, 2006, 1024, 1002, 1011, 3590, 1012, 5345, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "## Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set test size for splitting the data\n",
    "test_size = 0.1\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(X_resampled), list(y_resampled), test_size=test_size, random_state=42)\n",
    "\n",
    "# Extract text from the lists\n",
    "X_train = [text[0] for text in X_train]\n",
    "X_test = [text[0] for text in X_test]\n",
    "\n",
    "# Tokenize the training and testing data\n",
    "X_train_tokenized = tokenizer(X_train, padding=\"max_length\", truncation=True)\n",
    "X_test_tokenized = tokenizer(X_test, padding='max_length', truncation=True)\n",
    "\n",
    "# Create DataFrame for training data (optional, for inspection)\n",
    "x_train_df = pd.DataFrame(data={'text': X_train, 'labels': y_train})\n",
    "x_train_df.head()\n",
    "\n",
    "# Print a sample tokenized input for verification\n",
    "print(X_train_tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 11113,  2213, 10534,  1024,  1002,  1011,  2570,  1012,  1019,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(18)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a custom Dataset class\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "# Create Dataset objects for training and testing data\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "test_dataset = Dataset(X_test_tokenized, y_test)\n",
    "train_dataset[5] #structure sample of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing Model Layers\n",
    "Freezes the parameters of the first five layers of the DistilBERT model to prevent them from being updated during training. This can help in focusing the training on the later layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the first few layers of the model\n",
    "for layer in model.distilbert.transformer.layer[:5]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the evaluation metrics in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    print(type(p))\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Testing\n",
    "\n",
    "the notebook focuses on training and testing the model. It includes setting up training arguments, initializing the trainer, training the model, evaluating the model, and saving the trained model for future inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up the training arguments and initializes the `Trainer` object with the model, datasets, tokenizer, and evaluation metrics. This configuration controls various aspects of the training process, such as the number of epochs, batch size, logging, and evaluation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set up training arguments and initialize Trainer'''\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./output',  # Directory to store checkpoints and other training outputs\n",
    "    overwrite_output_dir=True, \n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size per device (GPU)\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy=\"epoch\",  # Evaluation is done (and logged) every epoch\n",
    "    logging_strategy=\"epoch\", # Log every epoch\n",
    "    logging_dir='./logs',  # Directory to store logs\n",
    "    lr_scheduler_type=\"linear\",  # Learning rate scheduler type\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0\n",
      "NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Clear CUDA cache and check device information Clears the CUDA cache to free up GPU memory and prints the current device information. \n",
    "This step ensures that the GPU is properly utilized and helps in debugging device-related issues.\n",
    "'''\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print device information\n",
    "print(trainer.args.device)\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 83/415 [00:27<01:26,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9268, 'grad_norm': 19.12451171875, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data analytics projects\\Financial Tracker\\FinTrackerenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\n",
      " 20%|â–ˆâ–ˆ        | 83/415 [00:29<01:26,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer_utils.EvalPrediction'>\n",
      "{'eval_loss': 1.0351004600524902, 'eval_accuracy': 0.6756756756756757, 'eval_precision': 0.6340740740740741, 'eval_recall': 0.6644444444444445, 'eval_f1': 0.6028245828245828, 'eval_runtime': 1.6264, 'eval_samples_per_second': 22.75, 'eval_steps_per_second': 3.074, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 166/415 [00:57<01:04,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6306, 'grad_norm': 4.949434280395508, 'learning_rate': 3e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data analytics projects\\Financial Tracker\\FinTrackerenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 166/415 [00:58<01:04,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer_utils.EvalPrediction'>\n",
      "{'eval_loss': 0.9256970286369324, 'eval_accuracy': 0.7567567567567568, 'eval_precision': 0.7444444444444444, 'eval_recall': 0.7311111111111112, 'eval_f1': 0.7081481481481482, 'eval_runtime': 1.6067, 'eval_samples_per_second': 23.029, 'eval_steps_per_second': 3.112, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 249/415 [01:26<00:43,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5038, 'grad_norm': 1.031240463256836, 'learning_rate': 2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data analytics projects\\Financial Tracker\\FinTrackerenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 249/415 [01:28<00:43,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer_utils.EvalPrediction'>\n",
      "{'eval_loss': 0.8508186936378479, 'eval_accuracy': 0.7297297297297297, 'eval_precision': 0.7222222222222221, 'eval_recall': 0.708888888888889, 'eval_f1': 0.6748148148148149, 'eval_runtime': 1.6135, 'eval_samples_per_second': 22.931, 'eval_steps_per_second': 3.099, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 332/415 [01:56<00:21,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4253, 'grad_norm': 2.7612881660461426, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data analytics projects\\Financial Tracker\\FinTrackerenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 332/415 [01:57<00:21,  3.81it/s]Checkpoint destination directory ./output\\checkpoint-332 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer_utils.EvalPrediction'>\n",
      "{'eval_loss': 0.8066428303718567, 'eval_accuracy': 0.7567567567567568, 'eval_precision': 0.7555555555555554, 'eval_recall': 0.7311111111111112, 'eval_f1': 0.7170370370370371, 'eval_runtime': 1.5961, 'eval_samples_per_second': 23.181, 'eval_steps_per_second': 3.133, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:25<00:00,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.368, 'grad_norm': 10.591322898864746, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data analytics projects\\Financial Tracker\\FinTrackerenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:27<00:00,  3.83it/s]Checkpoint destination directory ./output\\checkpoint-415 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer_utils.EvalPrediction'>\n",
      "{'eval_loss': 0.7969716191291809, 'eval_accuracy': 0.7567567567567568, 'eval_precision': 0.7555555555555554, 'eval_recall': 0.7311111111111112, 'eval_f1': 0.7170370370370371, 'eval_runtime': 1.6062, 'eval_samples_per_second': 23.035, 'eval_steps_per_second': 3.113, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [02:28<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 148.1999, 'train_samples_per_second': 11.1, 'train_steps_per_second': 2.8, 'train_loss': 0.5708873978580337, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=415, training_loss=0.5708873978580337, metrics={'train_runtime': 148.1999, 'train_samples_per_second': 11.1, 'train_steps_per_second': 2.8, 'train_loss': 0.5708873978580337, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.35it/s]e:\\Data analytics projects\\Financial Tracker\\FinTrackerenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer_utils.EvalPrediction'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7969716191291809,\n",
       " 'eval_accuracy': 0.7567567567567568,\n",
       " 'eval_precision': 0.7555555555555554,\n",
       " 'eval_recall': 0.7311111111111112,\n",
       " 'eval_f1': 0.7170370370370371,\n",
       " 'eval_runtime': 1.7189,\n",
       " 'eval_samples_per_second': 21.525,\n",
       " 'eval_steps_per_second': 2.909,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save the trained model and test inference'''\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Save the model to the specified directory\n",
    "trainer.save_model(r\"E:\\Data analytics projects\\Financial Tracker\\Model training\\Finance_categorization_fine_tuned\")\n",
    "\n",
    "# Load the saved model for inference\n",
    "model_2 = AutoModelForSequenceClassification.from_pretrained(r\"E:\\Data analytics projects\\Financial Tracker\\Model training\\Finance_categorization_fine_tuned\")\n",
    "\n",
    "# Define a text for classification\n",
    "text = \"POS Purchase FPOS SHELL C45051: $13.97\"\n",
    "\n",
    "# Initialize the pipeline for text classification\n",
    "pipe = pipeline(\"text-classification\", model=model_2, tokenizer=tokenizer)\n",
    "\n",
    "# Perform inference\n",
    "result = pipe(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Public transport': 11,\n",
       " 'Ciggerates': 4,\n",
       " 'Income': 8,\n",
       " 'Groceries': 7,\n",
       " 'Weed': 18,\n",
       " 'Restaurant, fast-food': 13,\n",
       " 'Child Support': 3,\n",
       " 'Interact': 9,\n",
       " 'Charges, Fees': 2,\n",
       " 'Unknown': 17,\n",
       " 'Alcohol': 0,\n",
       " 'Shopping': 14,\n",
       " 'Wellness, beauty': 19,\n",
       " 'Bill payments, Subscriptions': 1,\n",
       " 'Rent': 12,\n",
       " 'TRANSFER': 15,\n",
       " 'Education, development': 6,\n",
       " 'Phone, cell phone': 10,\n",
       " 'Drug-store, chemist': 5,\n",
       " 'Taxi': 16}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinTrackerenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
